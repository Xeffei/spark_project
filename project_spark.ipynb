{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "pr2YMY_i-Eb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f247e7-2dde-4713-e285-098d21c6c3d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=27e5d2b2f51ec3d4a405f847c48ec709282f324614489935f86fccf42e8426f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw6q6pQZ950R"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .getOrCreate()\n",
        "\n",
        "df = spark.read.format(\"csv\").load(\"housing.csv\", header=True, inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = df.randomSplit([0.7, 0.3])\n"
      ],
      "metadata": {
        "id": "8XNspUuaJYQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "empties = train.columns\n",
        "empties.remove('median_house_value')\n",
        "empties.remove('ocean_proximity')\n",
        "\n",
        "\n",
        "imputer = Imputer(inputCols=empties,\n",
        "                  outputCols=empties)\n",
        "\n",
        "imputer = imputer.fit(train)\n",
        "\n",
        "train = imputer.transform(train)\n",
        "test = imputer.transform(test)"
      ],
      "metadata": {
        "id": "IMmr4hsPJfTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "vector_asse = VectorAssembler(inputCols=empties,\n",
        "                                             outputCol='numeric_vector')\n",
        "\n",
        "train = vector_asse.transform(train)\n",
        "test = vector_asse.transform(test)"
      ],
      "metadata": {
        "id": "kz9W4ueGJjo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "\n",
        "scaler = StandardScaler(inputCol='numeric_vector',\n",
        "                        outputCol='scaled',\n",
        "                        withStd=True, withMean=True)\n",
        "\n",
        "scaler = scaler.fit(train)\n",
        "\n",
        "train = scaler.transform(train)\n",
        "test = scaler.transform(test)"
      ],
      "metadata": {
        "id": "PUns2FwPJm-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer = StringIndexer(inputCol='ocean_proximity',\n",
        "                        outputCol='ocean_index')\n",
        "\n",
        "indexer = indexer.fit(train)\n",
        "train = indexer.transform(train)\n",
        "test = indexer.transform(test)\n"
      ],
      "metadata": {
        "id": "VgUMdX75Jqxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(inputCol='ocean_index',\n",
        "                                outputCol='ocean_onehot')\n",
        "\n",
        "one_hot_encoder = one_hot_encoder.fit(train)\n",
        "\n",
        "train = one_hot_encoder.transform(train)\n",
        "test = one_hot_encoder.transform(test)\n"
      ],
      "metadata": {
        "id": "7FsVuqyAJvOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.show()"
      ],
      "metadata": {
        "id": "kNL0xO53D8u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(inputCols=['scaled',\n",
        "                                       'ocean_onehot'],\n",
        "                            outputCol='final')\n",
        "\n",
        "train = assembler.transform(train)\n",
        "test = assembler.transform(test)"
      ],
      "metadata": {
        "id": "uisgmVezJxh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.select(\"final\").show(truncate=False)"
      ],
      "metadata": {
        "id": "A3jvBfLnErn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "lr = LinearRegression(featuresCol='final',\n",
        "                      labelCol='median_house_value')\n",
        "lr = lr.fit(train)"
      ],
      "metadata": {
        "id": "_s0RBgdHJ0QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = lr.transform(test)\n",
        "sonuc = pred[['prediction','median_house_value']]\n",
        "\n",
        "sonuc_rdd = sonuc.rdd"
      ],
      "metadata": {
        "id": "nOCJCCcOJ8rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.evaluation import RegressionMetrics\n",
        "\n",
        "metrics = RegressionMetrics(sonuc_rdd)\n",
        "\n",
        "print(\"Mean Squared Error:\", metrics.meanSquaredError )\n",
        "print(\"Root Mean Squared Error:\", metrics.rootMeanSquaredError )\n",
        "print(\"Mean Absolute Error:\", metrics.meanAbsoluteError )\n",
        "print(\"R**2:\", metrics.r2 )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpNglWHiJ-qu",
        "outputId": "c5103198-2dc1-4aec-fd96-b88fedb4894d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 4396705654.4360695\n",
            "Root Mean Squared Error: 66307.65909332095\n",
            "Mean Absolute Error: 48623.43026733317\n",
            "R**2: 0.6696226253072244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "rfr = RandomForestRegressor(featuresCol='final',\n",
        "                      labelCol='median_house_value')\n",
        "model = rfr.fit(train)"
      ],
      "metadata": {
        "id": "m2mQm00PZZ-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.transform(test)\n",
        "sonuc = pred[['prediction','median_house_value']]\n",
        "\n",
        "sonuc_rdd = sonuc.rdd"
      ],
      "metadata": {
        "id": "oQnqk3Ftbvq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.evaluation import RegressionMetrics\n",
        "\n",
        "metrics = RegressionMetrics(sonuc_rdd)\n",
        "\n",
        "print(\"Mean Squared Error:\", metrics.meanSquaredError )\n",
        "print(\"Root Mean Squared Error:\", metrics.rootMeanSquaredError )\n",
        "print(\"Mean Absolute Error:\", metrics.meanAbsoluteError )\n",
        "print(\"R**2:\", metrics.r2 )"
      ],
      "metadata": {
        "id": "nkRFNoklbz5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "logic = LogisticRegression(featuresCol='final',\n",
        "                      labelCol='median_house_value')\n",
        "model2 = logic.fit(train)"
      ],
      "metadata": {
        "id": "4OM2ZIJ3TpDR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}